{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cfb67e8-3892-481d-a999-abd56c5c3a2f",
   "metadata": {},
   "source": [
    "### Bayesian Multi-Armed Bandit Mood Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "455953bf-47f6-4486-8feb-b066dbee442f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arm 0 pulled. Reward: 8.27, Total reward: 103.27223386365867\n",
      "Arm 1 pulled. Reward: 12.66, Total reward: 110.93661640547197\n",
      "Arm 2 pulled. Reward: -12.55, Total reward: 93.38452051984447\n",
      "Arm 3 pulled. Reward: 15.12, Total reward: 103.50838260907521\n",
      "Arm 0 pulled. Reward: 26.19, Total reward: 124.69724473137512\n",
      "Arm 0 pulled. Reward: 20.47, Total reward: 140.16261228609022\n",
      "Arm 0 pulled. Reward: -4.50, Total reward: 130.66742183430165\n",
      "Arm 0 pulled. Reward: -0.97, Total reward: 124.69844851867413\n",
      "Arm 0 pulled. Reward: 3.89, Total reward: 123.58640852057144\n",
      "Reward distributions changed: [ 2  4  0 -8]\n",
      "Arm 1 pulled. Reward: -39.73, Total reward: 78.8596726662811\n",
      "Arm 3 pulled. Reward: -21.26, Total reward: 52.601811123264106\n",
      "Arm 0 pulled. Reward: 3.54, Total reward: 51.139062182719385\n",
      "Arm 0 pulled. Reward: 2.58, Total reward: 48.721149367179386\n",
      "Arm 0 pulled. Reward: -9.43, Total reward: 34.291446388873155\n",
      "Arm 0 pulled. Reward: 5.58, Total reward: 34.86931999235599\n",
      "Arm 0 pulled. Reward: 7.61, Total reward: 37.47716525603833\n",
      "Arm 0 pulled. Reward: 12.83, Total reward: 45.3076776877911\n",
      "Arm 0 pulled. Reward: 12.54, Total reward: 52.84569820814013\n",
      "Arm 0 pulled. Reward: -11.78, Total reward: 36.06900452856922\n",
      "Reward distributions changed: [ 7 18  4 -5]\n",
      "Arm 0 pulled. Reward: -2.38, Total reward: 28.69075412941799\n",
      "Arm 0 pulled. Reward: 7.24, Total reward: 30.93007227407636\n",
      "Arm 0 pulled. Reward: 15.72, Total reward: 41.649212162200286\n",
      "Arm 0 pulled. Reward: 21.37, Total reward: 58.02277544553556\n",
      "Arm 0 pulled. Reward: 7.07, Total reward: 60.09583917294188\n",
      "Arm 0 pulled. Reward: 20.31, Total reward: 75.40465245266273\n",
      "Arm 0 pulled. Reward: 16.88, Total reward: 87.28667855770385\n",
      "Arm 0 pulled. Reward: 9.32, Total reward: 91.6096401394679\n",
      "Arm 0 pulled. Reward: 8.76, Total reward: 95.37144935999112\n",
      "Arm 0 pulled. Reward: -4.53, Total reward: 85.84579564339097\n",
      "Reward distributions changed: [-12  29   6  -4]\n",
      "Arm 0 pulled. Reward: -27.01, Total reward: 53.83811171131923\n",
      "Arm 0 pulled. Reward: -21.13, Total reward: 27.703986338095078\n",
      "Arm 0 pulled. Reward: -16.55, Total reward: 6.153654598219415\n",
      "Arm 0 pulled. Reward: -16.13, Total reward: -14.97396153368165\n",
      "Game over.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BayesianBandit:\n",
    "    def __init__(self, means, stds, pull_cost, no_op_cost, initial_reward, change_interval, initial_exploration=1):\n",
    "        self.means = np.array(means)\n",
    "        self.stds = np.array(stds)\n",
    "        self.pull_cost = pull_cost\n",
    "        self.no_op_cost = no_op_cost\n",
    "        self.initial_reward = initial_reward\n",
    "        self.n_arms = len(means)\n",
    "        self.total_reward = initial_reward\n",
    "        self.change_interval = change_interval\n",
    "        self.step_count = 0\n",
    "        self.counts = np.full(self.n_arms, initial_exploration)  # Start with some initial explorations\n",
    "        self.rewards = [np.random.normal(means[i], stds[i], initial_exploration).tolist() for i in range(self.n_arms)]  # Initial rewards\n",
    "        self.no_op_rewards = []  # Track rewards when choosing NO-OP\n",
    "\n",
    "    def pull(self, arm):\n",
    "        reward = np.random.normal(self.means[arm], self.stds[arm])\n",
    "        self.rewards[arm].append(reward)\n",
    "        self.total_reward += reward - self.pull_cost\n",
    "        return reward\n",
    "\n",
    "    def change_rewards(self):\n",
    "        self.means += np.random.randint(-20, 20, size=self.n_arms)\n",
    "        print(f\"Reward distributions changed: {self.means}\")\n",
    "\n",
    "    def choose_arm(self):\n",
    "        self.step_count += 1\n",
    "        if self.step_count % self.change_interval == 0:\n",
    "            self.change_rewards()\n",
    "\n",
    "        estimated_means = np.array([np.mean(rewards) for rewards in self.rewards])\n",
    "        estimated_std_devs = np.array([np.std(rewards) for rewards in self.rewards])\n",
    "        no_op_mean = np.mean(self.no_op_rewards) if self.no_op_rewards else -self.no_op_cost\n",
    "\n",
    "        #print(f\"Step {self.step_count}: Estimated Means: {estimated_means}, Estimated Std Devs: {estimated_std_devs}, NO-OP Mean: {no_op_mean}\")\n",
    "\n",
    "        if self.step_count <= self.n_arms:  # Force initial exploration\n",
    "            return self.step_count - 1\n",
    "\n",
    "        if no_op_mean > np.max(estimated_means - self.pull_cost):\n",
    "            return -1  # NO-OP\n",
    "        else:\n",
    "            return np.argmax(estimated_means)\n",
    "\n",
    "    def run(self):\n",
    "        while self.total_reward > 0:\n",
    "            arm = self.choose_arm()\n",
    "            if arm == -1:\n",
    "                self.total_reward -= self.no_op_cost\n",
    "                self.no_op_rewards.append(-self.no_op_cost)\n",
    "                print(f\"NO-OP chosen. Total reward: {self.total_reward}\")\n",
    "            else:\n",
    "                reward = self.pull(arm)\n",
    "                print(f\"Arm {arm} pulled. Reward: {reward:.2f}, Total reward: {self.total_reward}\")\n",
    "\n",
    "            if self.total_reward <= 0:\n",
    "                print(\"Game over.\")\n",
    "                break\n",
    "\n",
    "# Setup your bandit configuration\n",
    "bandit = BayesianBandit([5, 0, -10, 10], [10, 25, 20, 15], 5, 1, 100, 10)\n",
    "bandit.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001dcaee-1e82-4992-87e5-ef7708d0409e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
